{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObNy5hqLi5Obch69ns3LWw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Import Packages and Functions"
      ],
      "metadata": {
        "id": "RYVqowrvUMcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install python-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URPH8pIH1cUJ",
        "outputId": "2b7f9187-35ba-430c-d294-2b173404d3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-utils in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from python-utils) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9tlcurSJSi5"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "# # import utils\n",
        "# from public_tests import *\n",
        "# from test_utils import *\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Load Data"
      ],
      "metadata": {
        "id": "vhqL6Nri23gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/train-small.csv\")\n",
        "valid_df = pd.read_csv(\"/content/valid-small.csv\")\n",
        "\n",
        "test_df = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "jIXJblFv3cSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data types and null values check\n",
        "train_df.info()"
      ],
      "metadata": {
        "id": "vliz65vcU3wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.describe()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-g1yii-cU34b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore Data Labels"
      ],
      "metadata": {
        "id": "Fpqc88ZKXqx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create list of the names of each condition or disease\n",
        "columns = train_df.keys()\n",
        "columns = list(columns)\n",
        "columns"
      ],
      "metadata": {
        "id": "P8YFRIr4U4A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove unecessary elements\n",
        "columns.remove('Image')\n",
        "columns.remove('PatientId')\n",
        "\n",
        "# get total number of classes\n",
        "print(f\"Number of columns: {len(columns)} for these conditions: {columns}\")"
      ],
      "metadata": {
        "id": "Gbht4V75U4EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print number of positive labels for each class\n",
        "for column in columns:\n",
        "  print(f\"Class {column}: {train_df[column].sum()} samples\")"
      ],
      "metadata": {
        "id": "uhLweaHuU4Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = train_df.sum().drop(['Image', 'PatientId'])\n",
        "for column in class_counts.keys():\n",
        "  print(f\"The class {column} has {train_df[column].sum()} samples\")"
      ],
      "metadata": {
        "id": "QaxpA6pjokxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot up the distribution of counts\n",
        "sns.barplot(class_counts.values, class_counts.index, color='b')\n",
        "plt.title('Distribution of Classes for Training Dataset', fontsize=15)\n",
        "plt.xlabel('Number of Patients', fontsize=15)\n",
        "plt.ylabel('Diseases', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ZAvdzANoSBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check patient id\n",
        "print(f\"Total patient IDs: {train_df['PatientId'].count()}, Total unique IDs: {train_df['PatientId'].value_counts().shape[0]}\")"
      ],
      "metadata": {
        "id": "C1WwB-jpU39C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of unique patients is fewer that number of total patients\n",
        "\n",
        "To avoid having class imbalance impact the loss function is to weight the losses differently\n"
      ],
      "metadata": {
        "id": "jnMvXzRqXEHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Cardiomegaly',\n",
        "          'Emphysema',\n",
        "          'Effusion',\n",
        "          'Hernia',\n",
        "          'Infiltration',\n",
        "          'Mass',\n",
        "          'Nodule',\n",
        "          'Atelectasis',\n",
        "          'Pneumothorax',\n",
        "          'Pleural_Thickening',\n",
        "          'Pneumonia',\n",
        "          'Fibrosis',\n",
        "          'Edema',\n",
        "          'Consolidation']"
      ],
      "metadata": {
        "id": "vnnMZ6998TKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prevent Data Leakage\n",
        "Dataset contains multiple images for each patient\\\n",
        "Split is done on patient level do there is no 'leakage' between train, validation and test datasets."
      ],
      "metadata": {
        "id": "3HsmXcdu4gcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for leakage\n",
        "def check_for_leakage (df1, df2, patient_col):\n",
        "  # convert patient columns to sets of unique identifiers\n",
        "  df1_patients_unique = set(df1[patient_col])\n",
        "  df2_patients_unique = set(df2[patient_col])\n",
        "\n",
        "  # check for intersection between data sets\n",
        "  patients_in_both_groups = df1_patients_unique.intersection(df2_patients_unique)\n",
        "\n",
        "  # leakage contrains true if there is patient overlap, otherwise false\n",
        "  leakage = bool(patients_in_both_groups)\n",
        "\n",
        "  return leakage"
      ],
      "metadata": {
        "id": "5hsEFjQz4qx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"leakage between train and valid: {}\".format(check_for_leakage(train_df, valid_df, 'PatientId')))\n",
        "print(\"leakage between train and test: {}\".format(check_for_leakage(train_df, test_df, 'PatientId')))\n",
        "print(\"leakage between valid and test: {}\".format(check_for_leakage(valid_df, test_df, 'PatientId')))"
      ],
      "metadata": {
        "id": "pIIQKWYV5wef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Images\n",
        "- use `ImageDataGenerator` class\n",
        "- class supports basic data augmentation\n",
        "- transform batch values so that mean is 0 and s.deviation is 1 (standardizing input distribution)\n",
        "- convert grayscale images to 3-channel format (pre-trained model requires 3-channel input)"
      ],
      "metadata": {
        "id": "ZqHNMC-I54lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=8, seed=1, target_w = 320, target_h = 320):\n",
        "    \"\"\"\n",
        "    Return generator for training set, normalizing using batch\n",
        "    statistics.\n",
        "\n",
        "    Args:\n",
        "      train_df (dataframe): dataframe specifying training data.\n",
        "      image_dir (str): directory where image files are held.\n",
        "      x_col (str): name of column in df that holds filenames.\n",
        "      y_cols (list): list of strings that hold y labels for images.\n",
        "      batch_size (int): images per batch to be fed into model during training.\n",
        "      seed (int): random seed.\n",
        "      target_w (int): final width of input images.\n",
        "      target_h (int): final height of input images.\n",
        "\n",
        "    Returns:\n",
        "        train_generator (DataFrameIterator): iterator over training set\n",
        "    \"\"\"\n",
        "    print(\"getting train generator...\")\n",
        "    # normalize images\n",
        "    image_generator = ImageDataGenerator(\n",
        "        samplewise_center=True,\n",
        "        samplewise_std_normalization= True)\n",
        "\n",
        "    # flow from directory with specified batch size\n",
        "    # and target image size\n",
        "    generator = image_generator.flow_from_dataframe(\n",
        "            dataframe=df,\n",
        "            directory=image_dir,\n",
        "            x_col=x_col,\n",
        "            y_col=y_cols,\n",
        "            class_mode=\"raw\",\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            seed=seed,\n",
        "            target_size=(target_w,target_h))\n",
        "\n",
        "    return generator"
      ],
      "metadata": {
        "id": "kzfvQwjk6nos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build separate generator for valid and test sets\n",
        "- does not use batch statistics with test and validation data. images are processed one at a time\n",
        "- normalize incoming test data using statistics from training set"
      ],
      "metadata": {
        "id": "rQk1cb1Y7Mj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=100, batch_size=8, seed=1, target_w = 320, target_h = 320):\n",
        "    \"\"\"\n",
        "    Return generator for validation set and test set using\n",
        "    normalization statistics from training set.\n",
        "\n",
        "    Args:\n",
        "      valid_df (dataframe): dataframe specifying validation data.\n",
        "      test_df (dataframe): dataframe specifying test data.\n",
        "      train_df (dataframe): dataframe specifying training data.\n",
        "      image_dir (str): directory where image files are held.\n",
        "      x_col (str): name of column in df that holds filenames.\n",
        "      y_cols (list): list of strings that hold y labels for images.\n",
        "      sample_size (int): size of sample to use for normalization statistics.\n",
        "      batch_size (int): images per batch to be fed into model during training.\n",
        "      seed (int): random seed.\n",
        "      target_w (int): final width of input images.\n",
        "      target_h (int): final height of input images.\n",
        "\n",
        "    Returns:\n",
        "        test_generator (DataFrameIterator) and valid_generator: iterators over test set and validation set respectively\n",
        "    \"\"\"\n",
        "    print(\"getting train and valid generators...\")\n",
        "    # get generator to sample dataset\n",
        "    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n",
        "        dataframe=train_df,\n",
        "        directory=IMAGE_DIR,\n",
        "        x_col=\"Image\",\n",
        "        y_col=labels,\n",
        "        class_mode=\"raw\",\n",
        "        batch_size=sample_size,\n",
        "        shuffle=True,\n",
        "        target_size=(target_w, target_h))\n",
        "\n",
        "    # get data sample\n",
        "    batch = raw_train_generator.next()\n",
        "    data_sample = batch[0]\n",
        "\n",
        "    # use sample to fit mean and std for test set generator\n",
        "    image_generator = ImageDataGenerator(\n",
        "        featurewise_center=True,\n",
        "        featurewise_std_normalization= True)\n",
        "\n",
        "    # fit generator to sample from training data\n",
        "    image_generator.fit(data_sample)\n",
        "\n",
        "    # get test generator\n",
        "    valid_generator = image_generator.flow_from_dataframe(\n",
        "            dataframe=valid_df,\n",
        "            directory=image_dir,\n",
        "            x_col=x_col,\n",
        "            y_col=y_cols,\n",
        "            class_mode=\"raw\",\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "            target_size=(target_w,target_h))\n",
        "\n",
        "    test_generator = image_generator.flow_from_dataframe(\n",
        "            dataframe=test_df,\n",
        "            directory=image_dir,\n",
        "            x_col=x_col,\n",
        "            y_col=y_cols,\n",
        "            class_mode=\"raw\",\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "            target_size=(target_w,target_h))\n",
        "    return valid_generator, test_generator"
      ],
      "metadata": {
        "id": "GZh8tds27bcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_DIR = \"/content/images-small/\"\n",
        "train_generator = get_train_generator(train_df, IMAGE_DIR, \"Image\", labels)\n",
        "\n",
        "valid_generator, test_generator = get_test_and_valid_generator(valid_df, test_df, IMAGE_DIR, \"Image\", labels)\n"
      ],
      "metadata": {
        "id": "sO8S4-W87b15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`__get_item__(index)` function to look at what generator gives the model during training and validation"
      ],
      "metadata": {
        "id": "SrL8Jm638lPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = train_generator.__get_item__(0)\n",
        "plt.imshow(x[0]);"
      ],
      "metadata": {
        "id": "EWKXdifO7b_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Model Development"
      ],
      "metadata": {
        "id": "PB_-4Dvt9WiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Imbalance\n",
        "- ideally, model is trained using evely balanced dataset so that +ve and -ve training cases contribute equally to the loss\n",
        "- if normal cross-entropy loss function with higliy unbalanced dataset, algo will prioritize the majority class (ie, -ve in this case) since it contributes more to the loss\n",
        "> Contribution of each class (ie +ve or -ve):\n",
        "  1. freq(p) = No. +ve examples / N\n",
        "  2. freq(n) = No. -ve examples / N\n"
      ],
      "metadata": {
        "id": "367jTIC79bTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot frequency of labels in dataset\n",
        "plt.xticks(rotation=90)\n",
        "plt.bar(x=labels, height=np.mean(train_generator.labels, axis=0))\n",
        "plt.title(\"Frequency of Each Class\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xg19T8pQ7cF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate class fequencies"
      ],
      "metadata": {
        "id": "Wz8R4FTw9sa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_freqs(labels):\n",
        "    \"\"\"\n",
        "    Compute positive and negative frequencies for each class.\n",
        "\n",
        "    Args:\n",
        "        labels (np.array): matrix of labels, size (num_examples, num_classes)\n",
        "\n",
        "    Returns:\n",
        "        positive_frequencies (np.array): array of size (num_classes) with\n",
        "                                         the frequency of positive examples for each class\n",
        "        negative_frequencies (np.array): array of size (num_classes) with\n",
        "                                         the frequency of negative examples for each class\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the total number of patients (rows)\n",
        "    N = labels.shape[0]\n",
        "\n",
        "    # Compute positive frequencies\n",
        "    positive_frequencies = np.sum(labels, axis=0) / N\n",
        "\n",
        "    # Compute negative frequencies\n",
        "    negative_frequencies = 1 - positive_frequencies\n",
        "\n",
        "    return positive_frequencies, negative_frequencies"
      ],
      "metadata": {
        "id": "CpeNu-yj7chB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "compute frequencies for training data"
      ],
      "metadata": {
        "id": "McBi0__D-j5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_pos, freq_neg = compute_class_freqs(train_generator.labels)\n",
        "freq_pos"
      ],
      "metadata": {
        "id": "Is4uza3K9jkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualize contribution ratios for each of the pathologies"
      ],
      "metadata": {
        "id": "6x0dIPlR-x_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": freq_pos})\n",
        "data = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} for l,v in enumerate(freq_neg)], ignore_index=True)\n",
        "plt.xticks(rotation=90)\n",
        "f = sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data)"
      ],
      "metadata": {
        "id": "U1v7exzY9jvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- above plot shows that contribution of +ve classes is significantly lower than that of the -ve classes.\n",
        "- the contributions should be equal\n",
        "- one way to do this can be done by multiplying each example from each class by a class-specific weight factor (w-pos and w-neg) so that overall contribution of each class is the same\n",
        "> w(pos) * freq(p) = w(neg) * freq(n)\\\n",
        "> which can be done by taking: \\\n",
        "    1. w(pos) = freq(n)\n",
        "    2. w(neg) = freq(p)\n"
      ],
      "metadata": {
        "id": "A5eemTm-_Bze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weights = freq_neg\n",
        "neg_weights = freq_pos\n",
        "pos_contribution = freq_pos * pos_weights\n",
        "neg_contribution = freq_neg * neg_weights"
      ],
      "metadata": {
        "id": "dXnqY8EC9j2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graph contributions to verify this\n",
        "data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": pos_contribution})\n",
        "data = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v}\n",
        "                        for l,v in enumerate(neg_contribution)], ignore_index=True)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data);"
      ],
      "metadata": {
        "id": "rZ-qBIyc9kAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- above plot shows that, by applying these weightings, the +ve and -ve lavbels in each class would have the same aggregate contribution to the loss function.\n",
        "- implement loss function after computing weights"
      ],
      "metadata": {
        "id": "vom3-1zKxbAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Weighted Loss\n",
        "- loss function calculates the weightes loss for each batch.\n",
        "- multi-class loss: add avg loss for each individual class\n",
        "- add small value, e, to predicted values to avoid numerical error that would occur if the predicted values happen to be zero"
      ],
      "metadata": {
        "id": "whLLRrZixy6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    Return weighted loss function given negative weights and positive weights.\n",
        "\n",
        "    Args:\n",
        "      pos_weights (np.array): array of positive weights for each class, size (num_classes)\n",
        "      neg_weights (np.array): array of negative weights for each class, size (num_classes)\n",
        "\n",
        "    Returns:\n",
        "      weighted_loss (function): weighted loss function\n",
        "    \"\"\"\n",
        "    def weighted_loss(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Return weighted loss value.\n",
        "\n",
        "        Args:\n",
        "            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n",
        "            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n",
        "        Returns:\n",
        "            loss (float): overall scalar loss summed across all classes\n",
        "        \"\"\"\n",
        "        # initialize loss to zero\n",
        "        loss = 0.0\n",
        "\n",
        "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "        for i in range(len(pos_weights)):\n",
        "            # for each class, add average weighted loss for that class\n",
        "                loss += K.mean(-(pos_weights[i] *y_true[:,i] * K.log(y_pred[:,i] + epsilon)\n",
        "                             + neg_weights[i]* (1 - y_true[:,i]) * K.log( 1 - y_pred[:,i] + epsilon))) #complete this line, USE Keras functions instead of Numpy\n",
        "        return loss\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "    return weighted_loss"
      ],
      "metadata": {
        "id": "FdoWmCBuxZzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DenseNet121\n",
        "- pre-trained DenseNet121 model from Keras\n",
        "- add 2 layers on top:\n",
        "> 1. `GlobalAveragePooling2D` layer to get the avg of the last conv later from DenseNet121\n",
        "  2. `Dense` layer with `sigmoid` activation to get prediction logits for each class.\n",
        "\n",
        "- custom loss function in `compile` function"
      ],
      "metadata": {
        "id": "6WIOcbwhyRlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the base pre-trained model\n",
        "base_model = DenseNet121(weights='/content/densenet.hdf5', include_top=False)\n",
        "\n",
        "x = base_model.output\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# and a logistic layer\n",
        "predictions = Dense(len(labels), activation=\"sigmoid\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.compile(optimizer='adam', loss=get_weighted_loss(pos_weights, neg_weights))"
      ],
      "metadata": {
        "id": "9K66X8NLxZpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Training\n",
        "- training on small subset of data\n",
        "- ensure loss on training set is decreasing"
      ],
      "metadata": {
        "id": "YPxNVrcg1Es_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              validation_data=valid_generator,\n",
        "                              steps_per_epoch=100,\n",
        "                              validation_steps=25,\n",
        "                              epochs = 3)\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mTL2-tbAxZbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on larger dataset\n",
        "- option: load set of pre-trained weights\n",
        "- The model architecture for our pre-trained model is exactly the same, but we used a few useful Keras \"callbacks\" for this training. Do spend time to read about these callbacks at your leisure as they will be very useful for managing long-running training sessions:\n",
        "> 1. You can use `ModelCheckpoint` callback to monitor your model's `val_loss` metric and keep a snapshot of your model at the point.\n",
        "  2. You can use the `TensorBoard` to use the Tensorflow Tensorboard utility to monitor your runs in real-time.\n",
        "  3. You can use the `ReduceLROnPlateau` to slowly decay the learning rate for your model as it stops getting better on a metric such as `val_los`s to fine-tune the model in the final steps of training.\n",
        "  4. You can use the `EarlyStopping` callback to stop the training job when your model stops getting better in it's validation loss. You can set a `patience` value which is the number of epochs the model does not improve after which the training is terminated. This callback can also conveniently restore the weights for the best metric at the end of training to your model.\n",
        "  \n",
        "- (Read more about keras callbacks)"
      ],
      "metadata": {
        "id": "FXxgfejg1WTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"/content/pretrained_model.h5\")"
      ],
      "metadata": {
        "id": "88nmcoBl1V-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Prediction & Evaluation\n",
        "- use `predict_generator` function ot generate predictions for images in test set\n"
      ],
      "metadata": {
        "id": "CIgoWzTs2MwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_vals = model.predict_generator(test_generator, steps = len(test_generator))"
      ],
      "metadata": {
        "id": "ueXNwYbR1V6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC Curve and AUROC\n",
        "- ROC: Receiver Operating Characteristic curve\n",
        "- AUC: Area Under Curve\n",
        "\n",
        "- Simple terms: curve that is more to the left and the top has more 'area' under it indicates that the model is performing better.\n",
        "\n",
        "- sklearn functions:\n",
        "  - `roc_curve`\n",
        "  - `roc_auc_score`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "- compare performance to the AUCs reported in the original paper:\n",
        "  - [CheXNet](https://arxiv.org/abs/1711.05225)\n",
        "  - [CheXpert](https://arxiv.org/pdf/1901.07031)\n",
        "  - [CheXNeXt](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002686)\n",
        "\n"
      ],
      "metadata": {
        "id": "9PaP_KLn2ZIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auc_rocs = util.get_roc_curve(labels, predicted_vals, test_generator)"
      ],
      "metadata": {
        "id": "yytSABhz1V3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Learning with GradCAM\n",
        "- One challenge using DL in medicine:\n",
        "  - complex architecture used for neural networks makes them much harder to interpret compared to traditional ML models (eg linear models)\n",
        "- Class Actiation Maps (CAM):\n",
        "  - Common approach to increase the model interpretability.\n",
        "  - Useful to understand where the model is 'looking' when classifying an image\n",
        "\n",
        "\n",
        "- GradCAM technique:\n",
        "  - Produce heatmap highlighting the important regions in the image for predicting the pathological condition\n",
        "  - Done by extracting gradients of each predicted class\n",
        "  - Does not provide explanation of reasoning for each classification probability\n",
        "  - Still useful for 'debugging' model and augmenting prediction\n"
      ],
      "metadata": {
        "id": "GoqFPIBK3tdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load small training set and look at 4 classes with highest AUC measures\n",
        "df = pd.read_csv(\"/content/train-small.csv\")\n",
        "IMAGE_DIR = \"/content/images-small/\"\n",
        "\n",
        "# only show the labels with top 4 AUC\n",
        "labels_to_show = np.take(labels, np.argsort(auc_rocs)[::-1])[:4]"
      ],
      "metadata": {
        "id": "NDCtWEG31Vy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at specific images"
      ],
      "metadata": {
        "id": "6rNlh9W45mSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "util.compute_gradcam(model, '00008270_015.png', IMAGE_DIR, df, labels, labels_to_show)"
      ],
      "metadata": {
        "id": "jWKyhuzO1Vt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "util.compute_gradcam(model, '00011355_002.png', IMAGE_DIR, df, labels, labels_to_show)"
      ],
      "metadata": {
        "id": "RumXWPXK1VrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "util.compute_gradcam(model, '00029855_001.png', IMAGE_DIR, df, labels, labels_to_show)"
      ],
      "metadata": {
        "id": "mCOiNYOdxZRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "util.compute_gradcam(model, '00005410_000.png', IMAGE_DIR, df, labels, labels_to_show)"
      ],
      "metadata": {
        "id": "GCjFhaHK5sLb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}